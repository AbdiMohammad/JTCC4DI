- `PSNR`: Channel PSNR
- `dataset`: dataset to use for training. Can be `cifar10` or `imagenet`
- `model`: pretrained model to use. If `dataset` is `cifar10` then possible values are: [`resnet20`, `resnet32`, `resnet44`, `resnet56`, `resnet110`, `resnet1202`]. If `dataset` is `imagenet` then possible values are: [`resnet34`, `resnet50`, `resnet101`, `resnet152`]
- `cifar_pretrained_weights_path`: if dataset is cifar, you should specify the path for pretrained weights. You can downlaod the weights here: https://github.com/akamaster/pytorch_resnet_cifar10/tree/master
- `batch_size`, `n_epochs`: self-explainatory
- `pretrained_weights_path`: [optional] path to pretrained weights from a previous run of the program which created a modified version of resnet with codebooks etc. (weights saved using `save_weights_after_train_path`)
- `save_weights_after_train_path`: [optional] save the model parameters (fine-tuned resnet and codebook weights) to the specified path
- `codebook_lr`: learning rate used for codebook parameters
- `non_codebook_lr`: learning rate used for non-codebook parameters
- `output_folder`: path used to save training outputs (e.g. training and validation losses during training)

- `codebooks`: a list of codebook definitions each codebook has the following attributes: `layer`, `hidden_dim`, `codebook_size`, `beta`, `prune_epochs`, `prune_ns`
- `codebooks.layer`: specifies the layer of pretrained network after which the codebook is inserted. The syntax is the same python syntax that you would use in python to access the layer. For example suppose we want to insert a codebook after the first block of the first layer. We would access this block in python thusly: `model.layer1[0]`. So in ordr to insert a codebook after this layer we should set `layer` to be `layer1[0]`.
- `codebooks.hidden_dim`: the codebook embedding dimension
- `codebooks.codebook_size`: number of codebook hidden embeddings
- `codebooks.beta`: beta schedule for the codebook (beta is the entropy regularization factor). Can be `ConstantBeta(beta)` or `LinearBeta(begin_beta, end_beta)`.
- `prune_epochs`: epochs before which we prune the codebook
- `prune_ns`: the nth entry specifies the number of codebook entries to be pruned the nth time we prune this codebook
- `prune_channels`: specifies a list of channels to be gradually pruned during training. Each entry has the following attributes: `layer`, `prune_epochs`, `prune_ns`
- `prune_channels.layer`: similar to `codebooks.layer`
- `prune_epochs`: the epochs which we prune the channels
- `prune_ns`: the number of channels that are pruned in nth epoch. Note that due to an implementation detail this behaves differently from `codebooks.prune_ns` because `codebook.prune_ns` was cumulative. For example if `codebooks.prune_ns=[2, 2]`, after the second pruning there are `2+2=4` pruned codebooks. But here the prune ns are absolute. So for example if `prune_channels.prune_ns=[2, 4]`, after the second pruning 4 channels are pruned.
